#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Usage: {script_name} <dataset_path> <repositories.json> <output_dir>

Extract information about issues from DevGPT's *_file_sharings.json
in the <dataset_path>, aggregating data about ChatGPT conversations.
Aggregate information about projects (group by project).

From https://github.com/NAIST-SE/DevGPT/blob/main/README.md#github-code-file
- 'Type': Source type (always "code file")
- 'URL': URL to the mentioned source
  https://github.com/{{owner}}/{{repo}}/blob/{{commit_sha}}/{{file_path}}
  (for example "https://github.com/shazow/whatsabi/blob/f0f160aebcd698d4f58033b912f61a89090815c1/src/utils.ts")
- 'ObjectSha': Sha of the source
- 'FileName': Filename of this code file (basename)
  (for example "README.md")
- 'FilePath': Filepath to this code file (full path in repository)
  (for example "www/wip_new_website/README.md")
- 'Author': Author who introduced this mention (user name on GitHub)
- 'Content': Content of this code file, which can be decoded by base64
- 'RepoName': Name of the repository that contains this code file
  (for example "roc-lang/roc")
- 'RepoLanguage': Primary programming language of the repository that contains this code file
  NOTE: it can be null when this repository does not contain any code
  (for example "Assembly", "C", "Python", "CSS", null)
- 'CommitSha': Sha of the commit that introduced this mention
- 'CommitMessage': Message of the commit that introduced this mention
  (for example "Create README.md")
- 'AuthorAt': When the author added this mention
  (for example "2023-10-11T03:58:22Z")
- 'CommitAt': When the author committed this mention
  (same format as for 'AuthorAt')
- 'ChatgptSharing': *List* of ChatGPT link mentions.
  (here 'Mention.MentionedProperty' is always "code")


Example:
    python scripts/data/file_sharings_to_agg.py \\
        data/external/DevGPT/ data/repositories_download_status.json \\
        data/interim/
"""
import json
import sys
from os import PathLike
from pathlib import Path

import pandas as pd

from src.data.common import (load_repositories_json,
                             compute_chatgpt_sharings_stats, add_is_cloned_column)
from src.data.sharings import find_most_recent_file_sharings
from src.utils.functools import timed

# constants
ERROR_ARGS = 1
ERROR_OTHER = 2


def process_file_sharings(file_sharings_path, repo_clone_data):
    """Read file sharings, convert to dataframe, and aggregate over repos

    In DevGPT's GitHub Code File sharings, there is only a single non-scalar field,
    namely the 'ChaptgptSharing' field.  To convert ChatGPT sharing
    to dataframe, values contained in this field needs to be summarized into
    a few scalars (see docstring for :func:`compute_chatgpt_sharings_stats`).

    Additionally, an aggregate over repositories is computed, and also
    returned.  This aggregate dataframe included basic information about
    the repository, and the summary of the summary of non-scalar fields.

    :param PathLike file_sharings_path: path to file sharings JSON file
        from DevGPT dataset; the format of this JSON file is described in
        https://github.com/NAIST-SE/DevGPT/blob/main/README.md#github-code-file
    :param dict repo_clone_data: information extracted from <repositories.json>,
        used to add 'is_cloned' column to one of resulting dataframes
    :return: sharings aggregated over file (first dataframe),
        and over repos (second dataframe in the tuple)
    :rtype: (pd.DataFrame, pd.DataFrame)
    """
    with open(file_sharings_path) as file_sharings_file:
        file_sharings = json.load(file_sharings_file)

    if 'Sources' not in file_sharings:
        print(f"ERROR: unexpected format of '{file_sharings_path}'")
        sys.exit(ERROR_OTHER)

    file_sharings = file_sharings['Sources']
    compute_chatgpt_sharings_stats(file_sharings)

    # drop 'Contents', as it can be large, and is not well suited for analysis
    # it is content of code file with ChatGPT link, base64 encoded
    df_file = pd.DataFrame.from_records(file_sharings)
    # df_file.loc[:, 'ContentSizeBytes'] = df_file['Content'].str.decode('base64').str.len()
    df_file = df_file.drop(columns=['Content'])

    grouped = df_file.groupby(by=['RepoName'], dropna=False)
    df_repo = grouped.agg({
        'RepoLanguage': 'first',
        'URL': 'nunique',  # counts issues per repo
        'Sha': 'count',  # generated by retrieve_closing_commit_sha_for_issue(), should be unique
        **{
            col: 'sum'
            for col in [
                # from data added by compute_chatgpt_sharings_stats()
                'NumberOfChatgptSharings', 'Status404',
                'ModelGPT4', 'ModelGPT3.5', 'ModelOther',
                'TotalNumberOfPrompts', 'TotalTokensOfPrompts', 'TotalTokensOfAnswers',
                'NumberOfConversations',
            ]
        }
    })

    add_is_cloned_column(df_repo, repo_clone_data)

    return df_file, df_repo


@timed
def main():
    # handle command line parameters
    # {script_name} <dataset_path> <repositories.json> <output_dir>
    if len(sys.argv) != 3 + 1:  # sys.argv[0] is script name
        print(__doc__.format(script_name=sys.argv[0]))
        sys.exit(ERROR_ARGS)

    dataset_directory_path = Path(sys.argv[1])
    repositories_info_path = Path(sys.argv[2])
    output_dir_path = Path(sys.argv[3])

    # sanity check values of command line parameters
    if not dataset_directory_path.exists():
        print(f"ERROR: <dataset_path> '{dataset_directory_path}' does not exist")
        sys.exit(ERROR_ARGS)
    if not dataset_directory_path.is_dir():
        print(f"ERROR: <dataset_path> '{dataset_directory_path}' is not a directory")
        sys.exit(ERROR_ARGS)
    if not repositories_info_path.exists():
        print(f"ERROR: <repositories.json> '{repositories_info_path}' does not exist")
        sys.exit(ERROR_ARGS)
    if not repositories_info_path.is_file():
        print(f"ERROR: <repositories.json> '{repositories_info_path}' is not a file")
        sys.exit(ERROR_ARGS)
    if output_dir_path.exists() and not output_dir_path.is_dir():
        print(f"ERROR: <output_dir> '{output_dir_path}' exists and is not a directory")
        sys.exit(ERROR_ARGS)

    # ensure that <output_dir> exists
    if not output_dir_path.exists():
        output_dir_path.mkdir(parents=True, exist_ok=True)

    # .......................................................................
    # PROCESSING
    repo_clone_data = load_repositories_json(repositories_info_path)

    print(f"Finding sharings from DevGPT dataset at '{dataset_directory_path}'...",
          file=sys.stderr)
    file_sharings_path = find_most_recent_file_sharings(dataset_directory_path)
    print(f"Sharings for files at '{file_sharings_path}'", file=sys.stderr)

    # DO WORK
    file_df, repo_df = process_file_sharings(file_sharings_path, repo_clone_data)

    # write per-pr data
    issue_sharings_path = output_dir_path.joinpath('file_sharings_df.csv')
    print(f"Writing {file_df.shape} of per-file sharings data "
          f"to '{file_sharings_path}'", file=sys.stderr)
    file_df.to_csv(issue_sharings_path, index=False)
    # write per-repo data
    repo_sharings_path = output_dir_path.joinpath('file_sharings_groupby_repo_df.csv')
    print(f"Writing {repo_df.shape} of repo-aggregated file sharings data "
          f"to '{repo_sharings_path}'", file=sys.stderr)
    repo_df.to_csv(repo_sharings_path, index=True)



if __name__ == '__main__':
    main()

# end of file_sharings_to_agg.py
