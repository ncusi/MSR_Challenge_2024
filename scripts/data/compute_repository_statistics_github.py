#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Usage: {script_name} <dataset_path> <repositories.json>  <output.json>

Retrieves number of commits and authors from GitHub API with PyGithub

Example:
    python scripts/data/compute_repository_statistics_github.py \\
        data/DevGPT/ data/repositories_download_status.json \\
        data/repository_statistics_github.json
"""
import json
import sys
from pathlib import Path

from tqdm import tqdm

from src.data.sharings import recent_sharings_paths_and_repos
from src.utils.files import load_json_with_checks
from src.utils.functools import timed
from src.utils.github import get_Github, get_github_repo

# constants
ERROR_ARGS = 1
ERROR_OTHER = 2


def check_repositories_statistics(repositories_info_path, dataset_directory_path):
    """Extract information about projects from cloned git repositories

    The information extracted includes:
    TODO

    :param Path repositories_info_path: JSON file with information about cloned
        project repositories (generated by download_repositories.py script)
    :param Path dataset_directory_path: directory with downloaded DevGPT dataset
    :return: repository statistics retrieved from git, mapping from 'RepoName';
        suitable for converting to pandas DataFrame with pd.from_dict()
    :rtype: dict
    """
    repo_clone_info = load_json_with_checks(repositories_info_path,
                                            file_descr="<repositories.json>",
                                            data_descr="info about cloned repos",
                                            err_code=ERROR_ARGS, expected_type=list)

    print(f"Finding sharings from DevGPT dataset at '{dataset_directory_path}', extracting 'RepoName'...",
          file=sys.stderr)
    sharings_info = recent_sharings_paths_and_repos(dataset_directory_path)

    # NOTE: think about maybe switching to https://ghapi.fast.ai/
    print("Creating client for GitHub API with PyGithub...", file=sys.stderr)
    g = get_Github()

    results = {}
    for repo_info in tqdm(repo_clone_info, desc='cloned repository'):
        repo_name = repo_info['repository']

        # there should be no repeated repo_name, so no need for get_github_repo_cached()
        repo = get_github_repo(g, repo_name)
        if repo is None:
            tqdm.write(f"ERROR: no repository ({repo_name}) at {repo_info['repository_url']}")
            continue

        results[repo_name] = {
            'current_full_name': repo.full_name,  # might be different from `repo_name`
            'archived': repo.archived,
            'created_at': repo.created_at,
            'created_at_timestamp': int(repo.created_at.timestamp()),
            'updated_at': repo.updated_at,
            'updated_at_timestamp': int(repo.updated_at.timestamp()),
            'default_branch': repo.default_branch,
            'description': repo.description,
            'is_fork': repo.fork,
            'forks_count': repo.forks_count,
            'has_downloads': repo.has_downloads,
            'has_issues': repo.has_issues,
            'has_projects': repo.has_projects,
            'homepage': repo.homepage,
            'language': repo.language,
            'network_count': repo.network_count,
            'open_issues_count': repo.open_issues_count,
            'private': repo.private,
            'size': repo.size,
            'stargazers_count': repo.stargazers_count,
            'subscribers_count': repo.subscribers_count,
            'topics': repo.topics,  # list of strings
            #'repo_languages': repo.get_languages(),  # dict of string to integer
        }
        for sharings_type, sharings_data in sharings_info.items():
            results[repo_name][f"in_latest_{sharings_type}_sharings"] = \
                repo_name in sharings_data['repos']

    # close connections after use
    g.close()

    return results


@timed
def main():
    # handle command line parameters
    # {script_name} <dataset_path> <repositories.json> <output.json>
    if len(sys.argv) != 3 + 1:  # sys.argv[0] is script name
        print(__doc__.format(script_name=sys.argv[0]))
        sys.exit(ERROR_ARGS)

    dataset_dir_path = Path(sys.argv[1])
    repositories_info_path = Path(sys.argv[2])
    output_file_path = Path(sys.argv[3])

    data = check_repositories_statistics(repositories_info_path=repositories_info_path,
                                         dataset_directory_path=dataset_dir_path)

    # ensure that <output_file_path> can be created
    if not output_file_path.parent.exists():
        print(f"Creating '{output_file_path.parent}' directory...", file=sys.stderr)
        output_file_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"Writing output data to '{output_file_path}'...", file=sys.stderr)
    with open(output_file_path, 'w') as output_file:
        # default=str is here to avoid TypeError: Object of type datetime is not JSON serializable
        # https://stackoverflow.com/questions/11875770/how-can-i-overcome-datetime-datetime-not-json-serializable
        json.dump(data, output_file, indent=2, default=str)


if __name__ == '__main__':
    main()
