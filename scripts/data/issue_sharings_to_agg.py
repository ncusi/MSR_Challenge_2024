#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Usage: {script_name} <dataset_path> <repositories.json> <output_dir>

Extract information about issues from DevGPT's *_issue_sharings.json
in the <dataset_path>, aggregating data about ChatGPT conversations.
Aggregate information about projects (group by project).

From https://github.com/NAIST-SE/DevGPT/blob/main/README.md#github-issue
- 'Type': Source type (always "issue")
- 'URL': URL to the mentioned source
  (https://github.com/{{owner}}/{{repo}}/issues/{{issue_number}})
- 'Author': Author who introduced this mention (GitHub author, e.g. "tisztamo")
- 'RepoName': Name of the repository that contains this issue
  (full repo name: {{owner}}/{{repo}}, e.g. "dotCMS/core")
- 'RepoLanguage': Primary programming language of the repository that contains
  this pull request. NOTE: it can be null when this repository does not contain
  any code (e.g. "C++", "Python", "HTML",...)
- 'Number': Issue number of this issue (that is, {{issue_number}} in URL)
- 'Title': Title of this issue (e.g. "Implement proper error handling")
- 'Body': Description of this issue (might be empty, i.e. "")
- 'AuthorAt': When the author created this issue
  (e.g. "2023-09-16T06:02:27Z")
- 'ClosedAt': When this issue was closed
  NOTE: it can be null when this issue is not closed
- 'UpdatedAt': When the latest update of this issue occurred
- 'State': The state of this pull request (i.e., OPEN, CLOSED)
- 'ChatgptSharing':	A *list* of ChatGPT link mentions.

Example:
    python scripts/data/issue_sharings_to_agg.py \\
        data/external/DevGPT/ data/repositories_download_status.json \\
        data/interim/
"""
import json
import sys
from os import PathLike
from pathlib import Path

import pandas as pd
from tqdm import tqdm

from src.data.common import (load_repositories_json,
                             compute_chatgpt_sharings_stats, add_is_cloned_column)
from src.data.sharings import find_most_recent_issue_sharings
from src.utils.functools import timed
from src.utils.github import get_Github, get_github_repo_cached

# constants
ERROR_ARGS = 1
ERROR_OTHER = 2


def process_issue_sharings(issue_sharings_path, repo_clone_data):
    """Read issue sharings, convert to dataframe, and aggregate over repos

    In DevGPT's GitHub Issue sharings, there is only a single non-scalar field,
    namely the 'ChaptgptSharing' field.  To convert ChatGPT sharing
    to dataframe, values contained in this field needs to be summarized into
    a few scalars (see docstring for :func:`compute_chatgpt_sharings_stats`).

    TODO: Because there is no 'Sha' of commit that closes the issue,
    it needs to be found with the help of GitHub API.

    Additionally, an aggregate over repositories is computed, and also
    returned.  This aggregate dataframe included basic information about
    the repository, and the summary of the summary of non-scalar fields.

    :param PathLike issue_sharings_path: path to issue sharings JSON file
        from DevGPT dataset; the format of this JSON file is described in
        https://github.com/NAIST-SE/DevGPT/blob/main/README.md#github-issue
    :param dict repo_clone_data: information extracted from <repositories.json>,
        used to add 'is_cloned' column to one of resulting dataframes
    :return: sharings aggregated over pull request (first dataframe),
        and over repos (second dataframe in the tuple)
    :rtype: (pd.DataFrame, pd.DataFrame)
    """
    with open(issue_sharings_path) as issue_sharings_file:
        issue_sharings = json.load(issue_sharings_file)

    if 'Sources' not in issue_sharings:
        print(f"ERROR: unexpected format of '{issue_sharings_path}'")
        sys.exit(ERROR_OTHER)

    issue_sharings = issue_sharings['Sources']
    #retrieve_closing_commit_sha_for_issue(issue_sharings)
    compute_issue_state_stats(issue_sharings)
    compute_chatgpt_sharings_stats(
        issue_sharings,
        mentioned_property_values=['title', 'body', 'comments.body']
    )

    df_issue = pd.DataFrame.from_records(issue_sharings)

    grouped = df_issue.groupby(by=['RepoName'], dropna=False)
    df_repo = grouped.agg({
        'RepoLanguage': 'first',
        'Number': 'count',  # counts PR per repo
        #'Sha': 'count',  # generated by retrieve_closing_commit_sha_for_issue()
        **{
            col: 'sum'
            for col in [
                # from data added by compute_issue_state_stats()
                'StateOpen', 'StateClosed',
                # from data added by compute_chatgpt_sharings_stats()
                'NumberOfChatgptSharings', 'Status404',
                'ModelGPT4', 'ModelGPT3.5', 'ModelOther',
                'TotalNumberOfPrompts', 'TotalTokensOfPrompts', 'TotalTokensOfAnswers',
                'NumberOfConversations',
            ]
        }
    })

    add_is_cloned_column(df_repo, repo_clone_data)

    return df_issue, df_repo


def compute_issue_state_stats(issue_sharings):
    for source in tqdm(issue_sharings, desc="source ('State')"):
        if 'State' not in source:
            tqdm.write(f"warning: 'State' field is missing "
                       f"for '{source['RepoName']}' issue #{source['Number']}")
            continue

        issue_state = source['State']
        # it is a scalar field, we just one-hot encode it in addition

        source['StateOpen'] = bool(issue_state == 'OPEN')
        source['StateClosed'] = bool(issue_state == 'CLOSED')


def retrieve_closing_commit_sha_for_issue(issue_sharings):
    g = get_Github()
    github_repo_cache = {}

    print("Adding 'Sha' of issue-closing commit...", file=sys.stderr)
    for source in tqdm(issue_sharings, desc="source (finding 'Sha')"):
        repo_name = source['RepoName']

        repo = get_github_repo_cached(g, repo_name,
                                      repo_name, github_repo_cache)
        if repo is None:
            tqdm.write(f"WARNING: couldn't access repo on GitHub: {repo_name}")
            continue


@timed
def main():
    # handle command line parameters
    # {script_name} <dataset_path> <repositories.json> <output_dir>
    if len(sys.argv) != 3 + 1:  # sys.argv[0] is script name
        print(__doc__.format(script_name=sys.argv[0]))
        sys.exit(ERROR_ARGS)

    dataset_directory_path = Path(sys.argv[1])
    repositories_info_path = Path(sys.argv[2])
    output_dir_path = Path(sys.argv[3])

    # sanity check values of command line parameters
    if not dataset_directory_path.exists():
        print(f"ERROR: <dataset_path> '{dataset_directory_path}' does not exist")
        sys.exit(ERROR_ARGS)
    if not dataset_directory_path.is_dir():
        print(f"ERROR: <dataset_path> '{dataset_directory_path}' is not a directory")
        sys.exit(ERROR_ARGS)
    if not repositories_info_path.exists():
        print(f"ERROR: <repositories.json> '{repositories_info_path}' does not exist")
        sys.exit(ERROR_ARGS)
    if not repositories_info_path.is_file():
        print(f"ERROR: <repositories.json> '{repositories_info_path}' is not a file")
        sys.exit(ERROR_ARGS)
    if output_dir_path.exists() and not output_dir_path.is_dir():
        print(f"ERROR: <output_dir> '{output_dir_path}' exists and is not a directory")
        sys.exit(ERROR_ARGS)

    # ensure that <output_dir> exists
    if not output_dir_path.exists():
        output_dir_path.mkdir(parents=True, exist_ok=True)

    # .......................................................................
    # PROCESSING
    repo_clone_data = load_repositories_json(repositories_info_path)

    print(f"Finding sharings from DevGPT dataset at '{dataset_directory_path}'...",
          file=sys.stderr)
    issue_sharings_path = find_most_recent_issue_sharings(dataset_directory_path)
    print(f"Sharings for issues at '{issue_sharings_path}'", file=sys.stderr)

    # DO WORK
    issue_df, repo_df = process_issue_sharings(issue_sharings_path, repo_clone_data)

    # write per-pr data
    issue_sharings_path = output_dir_path.joinpath('issue_sharings_df.csv')
    print(f"Writing {issue_df.shape} of per-issue sharings data "
          f"to '{issue_sharings_path}'", file=sys.stderr)
    issue_df.to_csv(issue_sharings_path, index=False)
    # write per-repo data
    repo_sharings_path = output_dir_path.joinpath('issue_sharings_groupby_repo_df.csv')
    print(f"Writing {repo_df.shape} of repo-aggregated issue sharings data "
          f"to '{repo_sharings_path}'", file=sys.stderr)
    repo_df.to_csv(repo_sharings_path, index=True)


if __name__ == '__main__':
    main()
