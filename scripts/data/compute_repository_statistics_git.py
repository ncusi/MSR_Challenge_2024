#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Usage: {script_name} <dataset_path> <repositories.json>  <output.json>

Retrieves number of commits and authors from cloned project repositories

Example:
    python scripts/data/compute_repository_statistics_git.py \\
        data/DevGPT/ data/repositories_download_status.json \\
        data/repository_statistics_git.json
"""
import json
import subprocess
import sys
from pathlib import Path

from tqdm import tqdm

from src.data.sharings import recent_sharings_paths_and_repos
from src.utils.files import load_json_with_checks
from src.utils.functools import timed
from src.utils.git import GitRepo, select_core_authors, parse_shortlog_count

# constants
ERROR_ARGS = 1
ERROR_OTHER = 2


def check_repositories_statistics(repositories_info_path, dataset_directory_path):
    """Extract information about projects from cloned git repositories

    The information extracted includes:

    - 'author_number': number of different commit authors by name;
      found via 'git shortlog' starting from HEAD
    - 'commit_number': number of commits found starting from HEAD
    - 'commit_number_first_parent': like above, but following only the
      first parent of a commit; found via 'git log --first-parent'
    - 'files_number': number of files in the HEAD commit of the project
      repository; found with 'git ls-tree -r HEAD'
    - 'root_commit_number': number of root commits, that is commits
      which don't have a parent, found traversing from HEAD
    - 'HEAD_commit_timestamp': committer date as Unix timestamp for
      the HEAD commit
    - 'root_commit_timestamp': committer date as Unix timestamp for
      earliest root commit found traversing from HEAD
    - 'in_latest_commit_sharings', 'in_latest_pr_sharings', 'in_latest_issue_sharings':
      boolean value denoting whether given commit can be found in
      the relevant sharings file in most recent DevGPT shanpshot
      (checking 'RepoName')

    :param Path repositories_info_path: JSON file with information about cloned
        project repositories (generated by download_repositories.py script)
    :param Path dataset_directory_path: directory with downloaded DevGPT dataset
    :return: repository statistics retrieved from git, mapping from 'RepoName';
        suitable for converting to pandas DataFrame with pd.from_dict()
    :rtype: dict
    """
    repo_clone_info = load_json_with_checks(repositories_info_path,
                                            file_descr="<repositories.json>",
                                            data_descr="info about cloned repos",
                                            err_code=ERROR_ARGS, expected_type=list)

    print(f"Finding sharings from DevGPT dataset at '{dataset_directory_path}', extracting 'RepoName'...",
          file=sys.stderr)
    sharings_info = recent_sharings_paths_and_repos(dataset_directory_path)

    results = {}
    for repo_info in tqdm(repo_clone_info, desc='cloned repository'):
        repo_name = repo_info['repository']
        repository_path = Path(repo_info['repository_path'])
        if not repository_path.exists():
            # around 8 repositories were not possible to clone
            # (download_repositories.py asked for credentials)
            tqdm.write(f"WARNING: {repo_name} is not present at '{repository_path}'\n"
                       f"         was not cloned from {repo_info['repository_url']}")
            continue

        repo = GitRepo(repository_path)

        try:
            commit_number = repo.count_commits()
        except subprocess.CalledProcessError as err:
            # this means that repository is empty
            tqdm.write(f"WARNING: {repo_name} seems to have been empty when cloning\n"
                       f"         check at {repo_info['repository_url']}")
            if hasattr(err, 'stderr') and err.stderr:
                if isinstance(err.stderr, (bytes, bytearray)):
                    tqdm.write(f"{err.stderr.decode('utf8')}-----")
                else:
                    tqdm.write(f"{err.stderr}-----")
            continue

        commit_number_first_parent = repo.count_commits(first_parent=True)
        authors_list = repo.list_authors_shortlog()
        author_number = len(authors_list)
        core_authors_list, core_perc = select_core_authors(
            parse_shortlog_count(authors_list), perc=0.8)
        core_author_lumber = len(core_authors_list)
        files_number = len(repo.list_files())
        root_commits = repo.find_roots()
        HEAD_commit_timestamp = repo.get_commit_metadata('HEAD')['committer']['timestamp']
        root_commit_timestamp = min([
            repo.get_commit_metadata(commit)['committer']['timestamp']
            for commit in root_commits
        ])

        results[repo_name] = {
            'author_number': author_number,
            'core_author_number': core_author_lumber,
            'core_authors_commits_perc': core_perc,
            'commit_number': commit_number,
            'commit_number_first_parent': commit_number_first_parent,
            'files_number': files_number,
            'root_commit_number': len(root_commits),
            'HEAD_commit_timestamp': HEAD_commit_timestamp,
            'root_commit_timestamp': root_commit_timestamp,
        }
        for sharings_type, sharings_data in sharings_info.items():
            results[repo_name][f"in_latest_{sharings_type}_sharings"] = \
                repo_name in sharings_data['repos']

    return results


@timed
def main():
    # handle command line parameters
    # {script_name} <dataset_path> <repositories.json> <output.json>
    if len(sys.argv) != 3 + 1:  # sys.argv[0] is script name
        print(__doc__.format(script_name=sys.argv[0]))
        sys.exit(ERROR_ARGS)

    dataset_dir_path = Path(sys.argv[1])
    repositories_info_path = Path(sys.argv[2])
    output_file_path = Path(sys.argv[3])

    data = check_repositories_statistics(repositories_info_path=repositories_info_path,
                                         dataset_directory_path=dataset_dir_path)

    # ensure that <output_file_path> can be created
    if not output_file_path.parent.exists():
        print(f"Creating '{output_file_path.parent}' directory...", file=sys.stderr)
        output_file_path.parent.mkdir(parents=True, exist_ok=True)

    print(f"Writing output data to '{output_file_path}'...", file=sys.stderr)
    with open(output_file_path, 'w') as output_file:
        json.dump(data, output_file, indent=2)


if __name__ == '__main__':
    main()
